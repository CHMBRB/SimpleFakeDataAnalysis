{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data In, Data Out\n",
    "\n",
    "Software is generally useless without the ability to access underlying data stores whether they be in-memory, files, or databases. Data Analysis software maintains this truth.\n",
    "\n",
    "This exercise focuses primarily on fetching/storing data in various formats. Something which pandas (Panel Datas) + SQLAlchemy + openpyxl stack does efficiently.\n",
    "\n",
    "## The Trade-off\n",
    "\n",
    "SQLAlchemy is an **\"ORM\"** (Object Relational Mapper) which simplifies database connections for almost any source (SQLServer, MySQL, SQLite, and more). However, some of the operations translate to SQL operations which are expensive when compared with other alternatives. For example, it is generally advised when using SQL to avoid using cursor statements and that's exactly what happens at times. **Performance is the cost of convenience.**\n",
    "\n",
    "## Basic File Input/Output\n",
    "\n",
    "SQLAlchemy is not necessary for reading or writing to most file types with pandas since its primary use is interacting with databases. Generally, when we work with files it will be one of several types:\n",
    "\n",
    "- Excel\n",
    "- SQLite database\n",
    "- CSV\n",
    "- pickle\n",
    "- JSON\n",
    "- HTML\n",
    "- XML\n",
    "- HDF\n",
    "- etc...\n",
    "\n",
    "Pandas natively supports each type with respective methods and occasional library dependencies:\n",
    "- to_excel/read_excel **(with openpyxl)**\n",
    "- to_sql/read_sql **(with SQLAlchemy)**\n",
    "- to_csv/read_csv\n",
    "- to_pickle/read_pickle\n",
    "- to_json/read_json, etc...\n",
    "\n",
    "Each method may contain specific options unique to it despite similarities with other read/write methods.\n",
    "\n",
    "## First Steps\n",
    "\n",
    "The present methodology and related code may appear overly verbose when introducing new concepts. More common operations, variables, sets, etc... will be refactored to minimize the excessive amount of required (re)typing boilerplate code.\n",
    "\n",
    "## Our Second Code Block\n",
    "\n",
    "Our first code block printed \"Hello World\", but that was a stepping stone. The initial Code Block in many Jupyter notebooks will initialize, load, or otherwise instantiate the environment with:\n",
    "- required libraries\n",
    "- required variables\n",
    "- project constants\n",
    "\n",
    "We want to be as **\"DRY\"** (Don't Repeat Yourself) as possible while maintaining that the cells will execute in sequence without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import any required libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Writing CSV Files\n",
    "\n",
    "The presented code in the following block goes through several steps which produce a copy of the `Person.csv` file named `Person1.csv`.\n",
    "\n",
    "### Process Overview (Reinforcing Good Practices)\n",
    "\n",
    "Depending on an given individual's level of experience, there are rituals which we observe. We honor these rituals, not because we want to, but **because we must**. These rituals serve to benefit operations, reinforce accountability, and enhance our ability to audit errors that occur.\n",
    "\n",
    "As Programmers and Data Analysts, we strive for conventional, pragmatic wisdom. Writing code can be difficult and reviewing code weeks/months/years after it was writen may be nearly impossible without standardization and/or related processes. When possible **be explicit**, not **implicit**. The person most likely to benefit from these practices is *future* you.\n",
    "\n",
    "### Producing Backups\n",
    "\n",
    "1. A common first step when working with data files is to archive an unmodified backup of the file/directory. There are many reasons for this, but the most important is to **\"CYA\"** (Cover Your Ass) **WHEN** an error occurs. (An error or failure **WILL ALWAYS** occur no matter how much talent is present).\n",
    "\n",
    "2. Similarly, if working with databases, a backup should be created before running any script which attempts to DELETE, UPDATE, or INSERT records.\n",
    "    - It is otherwise possible to run a script which causes DATA LOSS unintentionally through coding errors\n",
    "    - **Never, ever, ever test a script on a Production Database**.\n",
    "        1. Instead, create a backup of the Production Database.\n",
    "        2. Import the backup into a Development Database.\n",
    "        3. Then test and inspect results.\n",
    "        4. Then have others, QA (Quality Assurance) or otherwise, inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following heading will be omitted from other code blocks, but it's provided as an example for others to quickly identify the code intent or other important metadata\n",
    "\n",
    "##\n",
    "#\n",
    "#   Script: Produce a copy of a csv in the same data directory\n",
    "#   Author: CHMBRB\n",
    "#   Date: 2023-11-22\n",
    "#   License: BSD-3-Clause\n",
    "#\n",
    "##\n",
    "\n",
    "# create a path object to person csv data (relative to the present notebook)\n",
    "person_csv = Path(\"../data/Person.csv\")\n",
    "\n",
    "# read the person csv which uses the tab-delimit character\n",
    "# NOTE: the low_memory flag option is generally required on Windows Systems for what is arbitrarily considered to be a \"large file\"\n",
    "df = pd.read_csv(person_csv, sep=\"\\t\", index_col='id', low_memory=False)\n",
    "# df = pd.read_csv(person_csv, **CONSTANT['csv_kwargs'])\n",
    "\n",
    "# declare a new relative csv document path\n",
    "person_csv_1 = Path(\"../data/Person1.csv\")\n",
    "\n",
    "# # NOTE: we could use the os library to produce a copy of the document without loading it into pandas and there's good reason to consider that approach over the one presented!\n",
    "# # instead, assume we did operations on the data and want to persist those changes in a new file, ie. The original file is the backup (not advised)\n",
    "# df.to_csv(person_csv_1, sep='\\t')\n",
    "\n",
    "# NOTE: additional code (written as a function) to perform file copy using shutil.copy to ensure that Pandas doesn't mangle the original file contents\n",
    "def copy_file(p1, p2):\n",
    "    \"\"\"Copy a file using the shutil library for two file system Path objects (p1, p2)\"\"\"\n",
    "    shutil.copy(p1, p2)\n",
    "\n",
    "\n",
    "# call the previously created function to produce a copy\n",
    "copy_file(person_csv, person_csv_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Writing SQLite Databases\n",
    "\n",
    "SQLite is a cross-platform application for persisting SQLite databases on local disk. It is a great tool for development despite several caveats and one potential advantage of using SQLite includes data manipulation with ANSI SQL (if team members are more comfortable using SQL).\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this example, we import the required SQLAlchemy dependency and instantiate a \"database engine\" using a connection string. Pandas then writes the entries to the specified table with several common method option keyword arguments (`**kwargs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection imports and setup\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# database connection string (this will vary for each database type and may require additional dependencies, drivers, etc...)\n",
    "# see also: https://docs.sqlalchemy.org/en/20/core/engines.html\n",
    "connection_string = \"sqlite:///../data/sqlite.db\"\n",
    "\n",
    "# instantiate the database engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# write the dataframe to the 'persons' database table\n",
    "df.to_sql(name=\"persons\", con=engine, if_exists='replace', index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Automation\n",
    "\n",
    "While we could utilize the approach previously presented to migrate data from multiple files, that may become tedious, acrobatic, and increase the likelihood for introducing errors. Plus, I dislike typing more than I have to unless there's an associated achievement award and there may be many more than just 3 CSV files to import. \n",
    "\n",
    "### Overview\n",
    "\n",
    "Instead of loading each individual CSV file into a DataFrame and then using the SQLAlchemy engine to write the data to a corresponding table, we'll construct a loop which iterates over the necessary files and performs the required migration sequence for each respective database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory which contains the relevant file set\n",
    "path_dir = Path(\"../data/\")\n",
    "\n",
    "# NOTE: this technique is common in python and referred to as a \"list comprehension\" or \"set comprehension\"; That's because it can be used to create new [] or {} objects (or other iterables)\n",
    "# here we collect all files ending with .csv EXCEPT for the one previously created to avoid duplicates\n",
    "# alternatively, the list comprehension could be a one-liner:\n",
    "# file_list = [x for x in path_dir.iterdir() if str(x).endswith('.csv') and str(x) != '../data/Person1.csv']\n",
    "file_list = [\n",
    "    # produce an iterator variable\n",
    "    x for x \n",
    "    # define the iterable to work on\n",
    "    in path_dir.iterdir() \n",
    "    # select files ending with '.csv' and EXCLUDE the copy produced earlier in this workbook\n",
    "    if str(x).endswith('.csv') and str(x) != '../data/Person1.csv'\n",
    "]\n",
    "\n",
    "# construct a loop, iterate through the files, and import the content to its respective sqlite database table\n",
    "for f in file_list:\n",
    "    # construct a pandas dataframe object from the current file\n",
    "    df = pd.read_csv(f, sep=\"\\t\", index_col='id', low_memory=False)\n",
    "    # strip the tablename from the filename and interpolate the results using f-strings (format string syntax)\n",
    "    table_name = f\"{str(f).split('/')[-1].split('.')[0].lower()}s\"\n",
    "    # write the dataframe to its respective table\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='replace', index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options\n",
    "\n",
    "At this point, if you and your team are more comfortable using SQL it is possible to query the database created from our CSV files using sqlitebrowser. Otherwise, we'll continue in future notebooks to evaluate data using Python + Pandas, explore creating relevant graphs with Matplotlib, and eventually use some of the related data to bind to PDF files (using PyPDF2) or generate tables and other original documents using Jinja + tabulate.\n",
    "\n",
    "## Reading SQL Statements\n",
    "\n",
    "Now that each table exists in our database, we can apply more complex SQL statements to pull the exact necessary datasets accordingly for additional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a relevant SQL statement to pull the necessary related entries\n",
    "sql_statement = \"\"\"SELECT *\n",
    "                    FROM persons p\n",
    "                    JOIN contactdetails c\n",
    "                        ON p.id == c.id\n",
    "                    JOIN labordetails l\n",
    "                        ON p.id == l.person_id;\"\"\"\n",
    "\n",
    "# execute sql statement against the database engine and read query results into a dataframe object\n",
    "df = pd.read_sql(sql_statement, con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What About *Real* Databases?\n",
    "\n",
    "The reason that I neglected to address accessing *real* databases is because it's a bit more nuanced. For example, there are typically licensing restrictions and potentially python database drivers which may have various setup requirements.\n",
    "\n",
    "To my knowledge, there are at least 2 common Microsoft SQL Server database drivers:\n",
    "- pyodbc\n",
    "- pymssql\n",
    "\n",
    "pyodbc is the __[Official Recommendation](https://learn.microsoft.com/en-us/sql/connect/python/python-driver-for-sql-server?view=sql-server-ver16)__.\n",
    "\n",
    "However, for this section we will make use of Postgres because it offers the most permissive licensing when compared with Microsoft SQL Server and MySQL.\n",
    "\n",
    "<!-- TODO: Create a docker compose file for this project's required services... -->\n",
    "\n",
    "<!-- Is this true? https://stackoverflow.com/questions/49888007/understanding-mysql-licensing -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Remarks\n",
    "\n",
    "After having completed this section, users should now be able to successfully import/export data using Python Pandas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-0AW4763D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
