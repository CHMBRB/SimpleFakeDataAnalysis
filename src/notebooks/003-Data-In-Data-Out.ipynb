{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data In, Data Out\n",
    "\n",
    "Software is generally useless without the ability to access underlying data stores whether they be in memory, files, or databases. Data Analysis software maintains this truth.\n",
    "\n",
    "This exercise focuses primarily on fetching/storing data in various formats. Something which pandas (Panel Datas) + SQLAlchemy + openpyxl do efficiently when working together.\n",
    "\n",
    "## The Trade-off\n",
    "\n",
    "SQLAlchemy is an **\"ORM\"** (Object Relational Mapper) that simplifies database connections for almost any source (SQLServer, MySQL, SQLite, and more). However, some of the operations translate to SQL operations which are expensive when compared with other alternatives. For example, it is generally advised when using SQL to avoid cursor statements and that's exactly what it relies upon. **Performance is the cost of convenience.**\n",
    "\n",
    "## Basic File Input/Output\n",
    "\n",
    "SQLAlchemy is not necessary for reading or writing to most file types with pandas since its primary use is interacting with databases. Generally, when we work with files it will be one of several types:\n",
    "\n",
    "- Excel\n",
    "- SQLite database\n",
    "- CSV\n",
    "- pickle\n",
    "- JSON\n",
    "- HTML\n",
    "- XML\n",
    "- HDF\n",
    "- etc...\n",
    "\n",
    "Pandas natively supports each type with respective methods and occasional library dependencies:\n",
    "- to_excel/read_excel **(with openpyxl)**\n",
    "- to_sql/read_sql **(with SQLAlchemy)**\n",
    "- to_markdown **(with tabulate)**\n",
    "- to_clipboard/read_clipboard **(with xclip, or xsel (with PyQt4 modules) on \\*nix)**\n",
    "- to_csv/read_csv\n",
    "- to_pickle/read_pickle\n",
    "- to_json/read_json\n",
    "- to_html/read_html\n",
    "- to_xml/read_xml, etc...\n",
    "\n",
    "Each method may contain specific options unique to it despite similarities in their methods.\n",
    "\n",
    "## First Steps\n",
    "\n",
    "The present methodology and related code may appear overly verbose when introducing new concepts. More common operations, variables, sets, etc... will be refactored to minimize the excessive amount of required (re)typing boilerplate code.\n",
    "\n",
    "## Our Second Code Block\n",
    "\n",
    "Our first code block printed \"Hello World\", but that was a stepping stone. The initial Code Block in many Jupyter notebooks will initialize, load, or otherwise instantiate the environment with:\n",
    "- required libraries\n",
    "- required variables\n",
    "- project constants\n",
    "\n",
    "We want to be as **\"DRY\"** (Don't Repeat Yourself) as possible while maintaining that the cells will execute in sequence deterministically without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import any required libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Writing CSV Files\n",
    "\n",
    "The presented code in the following block goes through several steps which produce a copy of the `Person.csv` file named `Person1.csv`.\n",
    "\n",
    "### Process Overview (Reinforcing Best Practices and Standardization)\n",
    "\n",
    "Depending on any given individual's level of experience there are rituals which we observe. We honor these rituals, not because we want to, but **because we must**. These rituals serve to benefit operations, reinforce accountability, and enhance our ability to audit/correct errors that may occur.\n",
    "\n",
    "As Programmers and Data Analysts, we strive for conventional, pragmatic wisdom. Writing code can be difficult and reviewing (or modifying) code weeks/months/years after it was writen may be nearly impossible without standardization efforts. When possible **be explicit**, not **implicit**. The person most likely to benefit from this practice is *future* you and not your replacement.\n",
    "\n",
    "### Producing Backups\n",
    "\n",
    "1. A common first step when working with data files is to archive an unmodified backup of any file/directory. There are many reasons for this, but the most important is to **\"CYA\"** (Cover Your Ass) **when** an error occurs. (An error or failure **WILL ALWAYS** occur eventually no matter how much talent is present).\n",
    "\n",
    "2. Similarly, if working with databases, a backup should be created before running any script which attempts to DELETE, UPDATE, or INSERT records.\n",
    "    - It is otherwise possible to run a script which causes DATA LOSS unintentionally through coding errors\n",
    "    - **Never, ever, ever test a script on a Production Database or File System Directory**.\n",
    "        1. Instead, create a backup of the Production Database or File System Directory.\n",
    "        2. Import the backup into a Development Database.\n",
    "        3. Then test and inspect results.\n",
    "        4. Then have others like QA (Quality Assurance) or other team members inspect the results.\n",
    "        5. Only proceed once entirely certain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following heading will be omitted from future blocks, but it's provided as an example for others to quickly identify the code intent or other important metadata\n",
    "\n",
    "##\n",
    "#\n",
    "#   Script: Produce a copy of a csv in the same data directory\n",
    "#   Author: chmbrb\n",
    "#   Date: 2023-11-22\n",
    "#   License: BSD-3-Clause\n",
    "#\n",
    "##\n",
    "\n",
    "# create a path object to person csv data (relative to the present notebook)\n",
    "person_csv = Path(\"../data/Person.csv\")\n",
    "\n",
    "# read the person csv which uses the tab-delimit character\n",
    "# NOTE: the low_memory flag option is generally required on Windows Systems for what is arbitrarily considered to be a \"large file\"\n",
    "df = pd.read_csv(person_csv, sep=\"\\t\", index_col='id', low_memory=False)\n",
    "# df = pd.read_csv(person_csv, **CONSTANT['csv_kwargs'])\n",
    "\n",
    "# declare a new relative csv document path\n",
    "person_csv_1 = Path(\"../data/Person1.csv\")\n",
    "\n",
    "# # NOTE: we could use the os library to produce a copy of the document without loading it into pandas and there's good reason to consider that approach over the one presented!\n",
    "# # instead, assume we did operations on the data and want to persist those changes in a new file, ie. The original file is the backup (not advised)\n",
    "# df.to_csv(person_csv_1, sep='\\t')\n",
    "\n",
    "# NOTE: additional code (written as a function) to perform file copy using shutil.copy to ensure that Pandas doesn't mangle the original file contents\n",
    "def copy_file(p1, p2):\n",
    "    \"\"\"Copy a file using the shutil library for two file system Path objects (p1, p2)\"\"\"\n",
    "    shutil.copy(p1, p2)\n",
    "\n",
    "\n",
    "# call the previously created function to produce a copy\n",
    "copy_file(person_csv, person_csv_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Writing SQLite Databases\n",
    "\n",
    "SQLite is a cross-platform application for persisting SQLite databases on local disk. It is a great tool for development despite several caveats and one potential advantage of using SQLite includes data manipulation with ANSI SQL (if team members are more comfortable with writing SQL queries).\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this example, we import the required SQLAlchemy dependency and instantiate a \"database engine\" using a connection string. Pandas then writes the entries to the specified table with several common method option keyword arguments (`**kwargs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database connection imports and setup\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# database connection string (this will vary for each database type and may require additional dependencies, drivers, etc...)\n",
    "# see also: https://docs.sqlalchemy.org/en/20/core/engines.html\n",
    "connection_string = \"sqlite:///../data/sqlite.db\"\n",
    "\n",
    "# instantiate the database engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# write the dataframe to the 'persons' database table\n",
    "df.to_sql(name=\"persons\", con=engine, if_exists='replace', index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Automation\n",
    "\n",
    "While it's possible to utilize the approach previously presented to migrate data from multiple files, it may become tedious, acrobatic, and increase the likelihood for introducing errors. Plus, it is not ideal and I dislike typing more than necessary. Also, there are no associated achievement awards and there may be many more than just 3 CSV files to import. \n",
    "\n",
    "### Overview\n",
    "\n",
    "Instead of loading each individual CSV file into a DataFrame and then using the SQLAlchemy engine to write the data to a corresponding table, we'll construct a loop which iterates over the necessary files and performs the required migration sequence for each respective database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory which contains the relevant file set\n",
    "path_dir = Path(\"../data/\")\n",
    "\n",
    "# NOTE: this technique is common in python and referred to as a \"list comprehension\" or \"set comprehension\"; That's because it can be used to create new [] or {} objects (or other iterables)\n",
    "# here we collect all files ending with .csv EXCEPT for the one previously created to avoid duplicates\n",
    "# alternatively, the list comprehension could be a one-liner:\n",
    "# file_list = [x for x in path_dir.iterdir() if str(x).endswith('.csv') and str(x) != '../data/Person1.csv']\n",
    "file_list = [\n",
    "    # produce an iterator variable\n",
    "    x for x \n",
    "    # define the iterable to work on\n",
    "    in path_dir.iterdir() \n",
    "    # select files ending with '.csv' and EXCLUDE the copy produced earlier in this workbook\n",
    "    if str(x).endswith('.csv') and str(x) != '../data/Person1.csv'\n",
    "]\n",
    "\n",
    "# construct a loop, iterate through the files, and import the content to its respective sqlite database table\n",
    "for f in file_list:\n",
    "    # construct a pandas dataframe object from the current file\n",
    "    df = pd.read_csv(f, sep=\"\\t\", index_col='id', low_memory=False)\n",
    "    # strip the tablename from the filename and interpolate the results using f-strings (format string syntax)\n",
    "    table_name = f\"{str(f).split('/')[-1].split('.')[0].lower()}s\"\n",
    "    # write the dataframe to its respective table\n",
    "    df.to_sql(name=table_name, con=engine, if_exists='replace', index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour to Renaming Files\n",
    "\n",
    "This is a situation that has occurred several times in my professional experience. A series of files were given formatted names seperated by some delimiter character (in this example it is an underscore), however, someone decided that they want the file names modified.\n",
    "\n",
    "While we could simply divert several people's efforts towards manually renaming the files for a few hours, that solution lacks swagger and may be costly. Instead we will automate using a script that would take at most about 10-15 minutes to write and a few seconds to test/run.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The following block will process a few text files and rename them accordingly with a simple python script. A base assumption of this exercise is that the files have a standard naming convention which will allow us to easily programmatically perform the update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "\n",
    "This solution makes use of built-in string object methods using the os library + pathlib Path objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the necessary files into a list\n",
    "file_list = [\n",
    "    x for x \n",
    "    in path_dir.iterdir() \n",
    "    if str(x).endswith('.txt')\n",
    "]\n",
    "\n",
    "for f in file_list:\n",
    "    # split apart the relevant fields and strip the directory name + file extension\n",
    "    (name, date, number) = str(f).split('/')[-1].split('.')[0].split('_')\n",
    "    # rename the file\n",
    "    os.rename(f, path_dir.joinpath(f\"{date}_{name}_{number}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: since the files are part of the source control we'll revert them back to their original names to avoid conflicts in the repository\n",
    "file_list = [x for x in path_dir.iterdir() if str(x).endswith('.txt')]\n",
    "\n",
    "for f in file_list:\n",
    "    # get the fields again\n",
    "    (date, name, number) = str(f).split('/')[-1].split('.')[0].split('_')\n",
    "    # revert the name\n",
    "    os.rename(f, path_dir.joinpath(f\"{name}_{date}_{number}.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2\n",
    "\n",
    "This solution primarily uses the pathlib library, Path object accessor methods, and should appear more succinct and intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [x for x in path_dir.iterdir() if str(x).endswith('.txt')]\n",
    "\n",
    "for f in file_list:\n",
    "    fname = f.stem.split('_')\n",
    "    new_file_name = f\"{fname[1]}_{fname[0]}_{fname[2]}{f.suffix}\"\n",
    "    f.rename(Path(f.parent, new_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and revert again\n",
    "file_list = [x for x in path_dir.iterdir() if str(x).endswith('.txt')]\n",
    "\n",
    "for f in file_list:\n",
    "    fname = f.stem.split('_')\n",
    "    new_file_name = f\"{fname[1]}_{fname[0]}_{fname[2]}{f.suffix}\"\n",
    "    f.rename(Path(f.parent, new_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options\n",
    "\n",
    "Returning to our actual data exercise. At this point, if you or members of your team are comfortable using SQL it is possible to query the database created from the CSV files using sqlitebrowser or the sqlite3 command line utility. Otherwise, we'll continue in future notebooks to evaluate data using Python + Pandas, explore creating relevant graphs with Matplotlib, and eventually use some of the related data to bind to PDF files (using pypdftk), generate tables, and/or other original documents using Jinja templates + tabulate.\n",
    "\n",
    "## Reading SQL Statements\n",
    "\n",
    "Now that each table exists in our database, we can apply more complex SQL statements to pull the specific necessary datasets into a DataFrame for additional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a relevant SQL statement to pull the necessary related entries\n",
    "sql_statement = \"\"\"SELECT *\n",
    "                    FROM persons p\n",
    "                    JOIN contactdetails c\n",
    "                        ON p.id == c.id\n",
    "                    JOIN labordetails l\n",
    "                        ON p.id == l.person_id;\"\"\"\n",
    "\n",
    "# execute sql statement against the database engine and read query results into a dataframe object\n",
    "df = pd.read_sql(sql_statement, con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What About *Actual* Production Databases?\n",
    "\n",
    "The reason access to *actual* databases was neglected is because it's a bit more nuanced. For example, there may be licensing restrictions and operating system specific python database driver installations that may have differ.\n",
    "\n",
    "To my knowledge, at least 2 common Microsoft SQL Server database drivers exist:\n",
    "- pyodbc\n",
    "- pymssql\n",
    "\n",
    "pyodbc is the __[Official Recommendation](https://learn.microsoft.com/en-us/sql/connect/python/python-driver-for-sql-server?view=sql-server-ver16)__ for Microsoft SQL Server.\n",
    "\n",
    "However, for this section we'll make use of Postgres because it offers the most permissive licensing when compared with other SQL Server products.\n",
    "\n",
    "The required python package for postgres is __[psycopg](https://pypi.org/project/psycopg/)__. Other, perhaps better, alternatives may exist.\n",
    "\n",
    "If your machine has docker installed, there is a related file in this project's root directory named `docker-compose.yml` which will provide an instance of postgres for use with this section. Otherwise, you may wish to install docker, install postgres, or bypass this section entirely.\n",
    "\n",
    "<!-- TODO: Create a docker compose file for this project's required services... -->\n",
    "\n",
    "<!-- Is this true? https://stackoverflow.com/questions/49888007/understanding-mysql-licensing -->\n",
    "\n",
    "## Repeating Processes\n",
    "\n",
    "This section will reproduce the above sqlite entries into a docker postgres database instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.sqlalchemy.org/en/20/dialects/postgresql.html#module-sqlalchemy.dialects.postgresql.psycopg\n",
    "# declare a valid postgres connection string with psycopg\n",
    "\n",
    "# these variables may need to be modified with credential information and/or other information to produce a valid connection string\n",
    "user_name = \"pgadmin\"\n",
    "user_password = \"SuperDevOpsPro9\"\n",
    "server_name = \"d00.local\"\n",
    "server_port = 5432\n",
    "database_name = \"defaultdb\"\n",
    "\n",
    "# NOTE: there are other methods of producing valid connection strings, but that is outside the scope of this document\n",
    "psql_connection_string = f\"postgresql+psycopg://{user_name}:{user_password}@{server_name}:{server_port}/{database_name}\"\n",
    "\n",
    "# instantiate the postgres engine\n",
    "psql_engine = create_engine(psql_connection_string)\n",
    "\n",
    "# gather the .csv files\n",
    "file_list = [x for x in path_dir.iterdir() if str(x).endswith('.csv') and str(x) != '../data/Person1.csv']\n",
    "\n",
    "# and so on and so forth...\n",
    "for f in file_list:\n",
    "    try:\n",
    "        df = pd.read_csv(f, sep=\"\\t\", index_col='id', low_memory=False)\n",
    "        table_name = f\"{str(f).split('/')[-1].split('.')[0].lower()}s\"\n",
    "        df.to_sql(name=table_name, con=psql_engine, if_exists='replace', index_label='id')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Remarks\n",
    "\n",
    "After having completed this section, users should now be able to successfully import/export data in various formats with python pandas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-0AW4763D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
