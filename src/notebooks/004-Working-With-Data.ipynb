{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Data\n",
    "\n",
    "Previously we've demonstrated how to import/export data using Python Pandas. In this section, we'll address some of the basics for working with in-memory datasets.\n",
    "\n",
    "## The DataFrame\n",
    "\n",
    "The **DataFrame** is an essential concept carried over from the **R Programming Language**. DataFrames efficiently store data in tabular arrays and/or matrices which Pandas leverages with the NumPy library to perform algebraic lambda array operations.\n",
    "\n",
    "## First Things First\n",
    "\n",
    "Until these materials have been reviewed and updated, I will intentionally repeat myself. I dislike typing more than required unless it:\n",
    "- reinforces learning\n",
    "- highlights perceived importance\n",
    "- holds some other significance\n",
    "\n",
    "So far we've noticed the beginning of a trend. We commonly employing the same libraries, functions, and reuse many of the same variables. Instead of continuing to manually import everything in each notebook, we'll author a script which ensures that `notebooks.app.utils` is available on `$PATH` for import.\n",
    "\n",
    "Simply stated, This code:\n",
    "```python\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# instantiate the required database connection\n",
    "connection_string = \"sqlite:///../data/sqlite.db\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# declare SQL statement\n",
    "sql_statement = \"\"\"SELECT * \n",
    "                    FROM persons p \n",
    "                    JOIN contactdetails c\n",
    "                        ON p.id == c.id \n",
    "                    JOIN labordetails l \n",
    "                        ON p.id == l.person_id;\"\"\"\n",
    "\n",
    "# instantiate the DataFrame object\n",
    "df = pd.read_sql(sql_statement, con=engine)\n",
    "\n",
    "# ... and so on and so forth\n",
    "```\n",
    "\n",
    "Will be refactored to use the imports from the app.utils module:\n",
    "\n",
    "```python\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "# determine current working directory location\n",
    "project_directory = path.abspath('')\n",
    "project_path = path.dirname(project_directory)\n",
    "\n",
    "# add current working directory location to $PATH\n",
    "if not project_path in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "# import the defined utils which contains a dictionary CONSTANT\n",
    "from notebooks.app.utils import *\n",
    "\n",
    "# get our variables from the CONSTANT dictionary\n",
    "sql_statement = CONSTANT['sql_query']\n",
    "sql_kwargs = CONSTANT['sql_kwargs']\n",
    "\n",
    "# instantiate the DataFrame object\n",
    "df = pd.read_sql(sql_statement, **sql_kwargs)\n",
    "\n",
    "# ... and so on and so forth\n",
    "```\n",
    "\n",
    "Now that it's obvious that the new beginning cell block is actually longer, Please look to the contents of the `notebooks.app.utils.globals` file. The greater efficiency pays out dividends over time in the form of not having to retype the various imports and variable declarations.\n",
    "\n",
    "### Notes on Syntax and an Internal Guide\n",
    "\n",
    "<!-- This section seems a bit personal, eh?\n",
    "how you gonna paraphrase bob ross properly? do the dude respect.\n",
    "how do one properly paraphrase until one performs proper research?\n",
    "why has 'one'(you) been subjectively limited in use either/or in 'liberal studies'/english? \n",
    "-->\n",
    "Humans are inconsistent. The author is human despite repeated attempts to transcend. Humans are flawed/flaws and that's okay. In the wise and loosely paraphrased words of Bob Ross: \"We don't make mistakes, just happy little accidents.\" \n",
    "\n",
    "A list of notable items:\n",
    "\n",
    "- CAPITAL CASE IS A COMMON CONVENTION FOR CONSTANT VARIABLE DEFINITIONS IN MANY PROGRAMMING LANGUAGES.\n",
    "- snake_case is convention in Python due to the [PEPs](https://peps.python.org/pep-0008/). PascalCase naming and camelCase are other common conventions, but not common in Python.\n",
    "    - preceded by a single '\\_' or two '\\__' indicates intended OOP 'privacy' variables in interpreted and/or web languages (generally). They can still be directly accessed, but *shouldn't*.\n",
    "    - duck-typed languages and polymorphism offer convenience w/ implied limitations (usually involving performance and optimization, but on the positive they're great for prototyping since there's no compile time!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, we'll define a script to be used in the first block of each notebook to ensure that app.utils will be added to the project PATH\n",
    "# Once app.utils is on project path we will be able to import boilerplate Python code & variables\n",
    "\n",
    "## TODO: SECTION\n",
    "# insert boilerplate workaround courtesy of stack overflow\n",
    "# https://stackoverflow.com/questions/61058798/python-relative-import-in-jupyter-notebook\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "project_directory = path.abspath('')\n",
    "project_path = path.dirname(project_directory)\n",
    "\n",
    "if not project_path in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "from notebooks.app.utils import *\n",
    "## TODO: END SECTION\n",
    "\n",
    "# construct our working DataFrame\n",
    "# NOTE: the ** operator is a python operation to unpack a dictionary object into **kwargs syntax whereas the * prefix is used to unpack a list object for *args syntax\n",
    "# this means that each argument is deconstructed to k=v in the method\n",
    "df = pd.read_sql(CONSTANT['sql_query'], **CONSTANT['sql_kwargs'])\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data, Viewing Data, and Other General Information\n",
    "\n",
    "It's common that not all columns are required for analysis. Rather than clutter our workspace with unnecessary data, it's best to limit the scope of what is available for inspection.\n",
    "\n",
    "### Overview\n",
    "\n",
    "In the following block, columns will be removed, the data set reindexed, and entries assigned to a new DataFrame object which omits the payroll entries. Then we'll demonstrate some of the more common Pandas methods for viewing data or gathering other general information statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASICS: selecting data, viewing data, and general information statistics about a dataset\n",
    "\n",
    "# not ALL of the columns are necessary, so lets SELECT a few columns to work with from the original DataFrame\n",
    "# NOTE: feel free to experiment! customizing the columns section is essential to proper understanding!!! Live a little!!! c'mon!!!!\n",
    "select_columns = [\n",
    "    'lname',\n",
    "    'fname',\n",
    "    'email',\n",
    "    'address',\n",
    "    'city',\n",
    "    'state',\n",
    "    'zipcode',\n",
    "    'dob',\n",
    "    # 'ssn',\n",
    "    'payrate',\n",
    "    # 'current_date',\n",
    "    # 'time_in',\n",
    "    # 'time_out',\n",
    "]\n",
    "\n",
    "# lets also define columns from each specific data table source (in case we want to persist any changes to the database)\n",
    "person_columns = [\"id\", \"fname\", \"lname\", \"job\", \"payrate\", \"ssn\", \"dob\"]\n",
    "\n",
    "contact_columns = [\"id\", \"phone\", \"email\", \"address\", \"city\", \"state\", \"zipcode\"]\n",
    "\n",
    "labor_columns = [\"id\", \"person_id\", \"current_date\", \"time_in\", \"time_out\"]\n",
    "\n",
    "\n",
    "# create a copy using the previously defined column name filter\n",
    "ndf = df[select_columns].copy()\n",
    "\n",
    "# remove duplicate entries aka SELECT DISTINCT in SQL\n",
    "ndf.drop_duplicates(inplace=True)\n",
    "\n",
    "# re-index the dataset\n",
    "ndf.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# print the top 5 rows\n",
    "ndf.head()\n",
    "\n",
    "# print the bottom 10 rows\n",
    "# NOTE: df.head() also accepts an integer as an argument\n",
    "ndf.tail(10)\n",
    "\n",
    "# print unique field values\n",
    "ndf.nunique()\n",
    "\n",
    "# print general statistics (for numeric datatypes)\n",
    "ndf.describe()\n",
    "\n",
    "# # similar and/or related methods\n",
    "# ndf.min()\n",
    "# ndf.max()\n",
    "# ndf.mean()\n",
    "# ndf.median()\n",
    "# ndf.mode()\n",
    "# ndf.std()\n",
    "# ndf.sum()\n",
    "# ndf.cumsum()\n",
    "# ndf.cumprod()\n",
    "\n",
    "\n",
    "# check for nulls\n",
    "ndf.isna()\n",
    "ndf.notna()\n",
    "\n",
    "ndf.isnull()\n",
    "ndf.notnull()\n",
    "\n",
    "# replace null values\n",
    "ndf.fillna(0)\n",
    "\n",
    "# count the number of entries\n",
    "ndf['payrate'].count()\n",
    "\n",
    "# print current column datatypes\n",
    "ndf.dtypes\n",
    "\n",
    "# print the column names\n",
    "ndf.columns\n",
    "\n",
    "# verify dataframe memory usage\n",
    "# ndf.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Filters, Sorting, and Operators\n",
    "\n",
    "To review common operands, refer to 001-Python-101-For-The-Uninitiated.ipynb#operation-table\n",
    "\n",
    "<!-- TODO: Link to the file section -->\n",
    "\n",
    "<!-- [Link](./notebooks/001-Python-101-For-The-Uninitiated#operation-table) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying filters and sorting!\n",
    "\n",
    "# create a filter expression to check for entries which match a condition\n",
    "payrate_filter_expression = ndf['payrate'] > 30\n",
    "\n",
    "# create a slightly more complex filter expression\n",
    "# NOTE: Use of parentheses; without them a TypeError mismatch will occur concerning comparison of int and str using &\n",
    "complex_filter_expression = (ndf['payrate'] > 30) & (ndf['state'] == 'Arizona')\n",
    "\n",
    "# apply the filter to our data set\n",
    "ndf[payrate_filter_expression]\n",
    "\n",
    "# apply a slightly more complex filter\n",
    "ndf[complex_filter_expression]\n",
    "\n",
    "# or apply the inverse filter using the unary operator symbol '~' (tilde)\n",
    "# NOTE: The unary operator may not function as intended with complex_filter_expression\n",
    "ndf[~payrate_filter_expression]\n",
    "\n",
    "# count the number of entries resulting from the filter\n",
    "ndf['payrate'][payrate_filter_expression].count()\n",
    "\n",
    "# re-sort the array on any given column(s)\n",
    "# this is written across multiple lines enclosed in brackets/parentheses to allow for easily commenting and re-sorting the sort_values filter columns\n",
    "# TODO: have fun! live a little and experiment!\n",
    "ndf.sort_values(\n",
    "    by=[\n",
    "        # 'state',\n",
    "        'lname',\n",
    "        # 'payrate',\n",
    "        'dob',\n",
    "    ],\n",
    "    ascending=True,\n",
    ")\n",
    "\n",
    "# ndf[ndf.notna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dilemma\n",
    "\n",
    "If it's decided that the current column names are undesirable, then it's possible to manipulate them.\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this section, we'll create a new DataFrame named `odf` in which columns will be renamed and existing column data will be used to produce new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "\n",
    "# define a dictionary (or mapping) of key value pairs which represent the current column name and the anticipated column name\n",
    "new_column_names = {\n",
    "    \"lname\": \"last_name\",\n",
    "    \"fname\": \"first_name\",\n",
    "    \"email\": \"email_address\",\n",
    "    \"zipcode\": \"postal_code\",\n",
    "    \"dob\": \"date_of_birth\",\n",
    "    \"payrate\": \"rate_of_pay\",\n",
    "}\n",
    "\n",
    "odf = ndf.rename(columns=new_column_names)\n",
    "\n",
    "odf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new columns and using existing data in calculations\n",
    "\n",
    "# we want a column named 'full_name' which displays the employee's name in the form of \"last_name, first_name\" in all caps\n",
    "odf['full_name'] = odf['last_name'].str.upper() + \", \" + odf['first_name'].str.upper()\n",
    "\n",
    "# we decide to give the employees a 50 cent raise because of a raise in the cost of living, to share profits, incentivize, or because 'nobody wants to work'\n",
    "\n",
    "# define a value to represent the raise amount\n",
    "raise_amt = .5\n",
    "\n",
    "\n",
    "# apply the amount to the entire workforce!\n",
    "odf['rate_of_pay'] = odf['rate_of_pay'] + raise_amt\n",
    "\n",
    "\n",
    "# uh-oh.... management isn't happy, they only wanted raises to be given to employees in specific regions to 'enhance competitiveness' (whatever that means)\n",
    "# first, we'll undo our changes (in-memory)\n",
    "odf['rate_of_pay'] = odf['rate_of_pay'] - raise_amt\n",
    "\n",
    "# now we'll try again with a list curated by our benevolent dictators including the specific competitive states/regional employees that should be incentivized\n",
    "select_states = [\n",
    "    \"Arizona\",\n",
    "    \"California\",\n",
    "    \"New York\",\n",
    "]\n",
    "\n",
    "# TODO\n",
    "# approach A)\n",
    "# it works, but triggers a warning!\n",
    "# odf['rate_of_pay'][odf['state'].isin(select_states)] = odf['rate_of_pay'] + raise_amt\n",
    "\n",
    "\n",
    "# approach B)\n",
    "# this also triggers the warning!\n",
    "\n",
    "mask = odf[odf['state'].isin(select_states)]\n",
    "odf['rate_of_pay'].loc[[x for x in mask.index]] = odf['rate_of_pay'] + raise_amt\n",
    "\n",
    "# TODO: Review and correct to not trigger the associated warning and properly batch updates with filters!!!\n",
    "\n",
    "# odf.loc[[x for x in mask.index]]\n",
    "\n",
    "# mask = odf['state'].isin(select_states)\n",
    "# odf.where(mask)\n",
    "\n",
    "# odf[odf['state'].isin(select_states)]\n",
    "# mask\n",
    "\n",
    "# odf.loc[:, odf['state'].isin(select_states)]\n",
    "\n",
    "# odf.loc[:, mask['rate_of_pay']]\n",
    "\n",
    "# odf.loc[:, mask]\n",
    "\n",
    "# mask[['rate_of_pay', 'full_name', 'state']]\n",
    "\n",
    "# odf.iloc[mask['rate_of_pay']]\n",
    "\n",
    "# odf.loc[mask['rate_of_pay']]\n",
    "\n",
    "\n",
    "\n",
    "# odf.loc[mask['rate_of_pay'], :]\n",
    "# odf.loc[:, mask['rate_of_pay']]\n",
    "\n",
    "# odf[odf['state'].isin(select_states)]['rate_of_pay'] = odf['rate_of_pay'] + raise_amt\n",
    "\n",
    "# odf['rate_of_pay'][odf['state'].isin(select_states)] = odf['rate_of_pay'] - raise_amt\n",
    "\n",
    "\n",
    "# odf['rate_of_pay'][odf['state'].isin(select_states)]\n",
    "\n",
    "# since o\n",
    "#  \n",
    "\n",
    "\n",
    "# odf.nunique()\n",
    "# odf['state'].filter(items=select_states)\n",
    "# odf['state'].filter(items=select_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then it's realized that the employee email addresses are managed by a corporate account in a specific format, etc...\n",
    "# so lets update the employee email addresses!\n",
    "\n",
    "odf['email_address'] = odf['first_name'].str.lower() + \".\" + odf['last_name'].str.lower() + '@example.org'\n",
    "\n",
    "\n",
    "# it's now decided that the date_of_birth is unnecessary, but for compliance reasons the individual's age in years is needed\n",
    "\n",
    "# cast date of birth to a date datatype\n",
    "# import datetime library to generate current timestamp\n",
    "# NOTE: age is actually difficult to calculate due to leapyears\n",
    "# for this there is a third-party module dateutil which will be aliased for further demonstration\n",
    "from datetime import date\n",
    "# NOTE: an example of aliasing a library; python-dateutil is the actual package name\n",
    "from dateutil.relativedelta import relativedelta as rd\n",
    "\n",
    "\n",
    "# lets be explicit (not implicit) by specifying and converting the data types\n",
    "\n",
    "# optionally, lets explicitly cast each column to their respective datatype\n",
    "# where and when possible use numpy datatypes for accuracy\n",
    "# NOTE: remember, be explicit rather than implicit\n",
    "# https://stackoverflow.com/questions/15891038/change-column-type-in-pandas\n",
    "column_data_types = {\n",
    "    \"last_name\": np.str_,\n",
    "    \"first_name\": np.str_,\n",
    "    \"email_address\": np.str_,\n",
    "    \"address\": np.str_,\n",
    "    \"city\": np.str_,\n",
    "    \"state\": np.str_,\n",
    "    \"postal_code\": np.str_,\n",
    "    \"date_of_birth\": 'datetime64[ns]',\n",
    "    \"rate_of_pay\": np.double,\n",
    "    \"full_name\": np.str_,\n",
    "}\n",
    "\n",
    "odf = odf.astype(column_data_types)\n",
    "\n",
    "# what day is it?\n",
    "today = date.today()\n",
    "\n",
    "# NOTE: expand on the use of apply/map and the lambda statement (since this is its first appearance)\n",
    "odf['age'] = odf['date_of_birth'].apply(lambda x: rd(today, x).years)\n",
    "\n",
    "odf[['full_name','age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel/common error; leading zeroes are truncated\n",
    "# lets restore them!\n",
    "filter_exp = odf[odf['postal_code'].str.len() < 5]\n",
    "\n",
    "odf['postal_code'][filter_exp] = odf['postal_code'].apply(lambda x: f\"0{x}\")\n",
    "\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html\n",
    "# pd.pivot_table(odf, values=, index=[], columns=[])\n",
    "\n",
    "# pd.pivot_table(odf, values)\n",
    "# odf.head()\n",
    "# table = pd.pivot_table(odf, values=['age'], index=['full_name', 'date_of_birth'], aggfunc={'age': 'sum'})\n",
    "table = pd.pivot_table(odf, values=['age'], index=['full_name', 'date_of_birth'], aggfunc={'age': 'sum'})\n",
    "table\n",
    "# table = pd.pivot_table(odf, values=['age'], index=['full_name', 'date_of_birth'], columns=[], aggfunc={'age': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling data\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\n",
    "# df.dtypes\n",
    "df.current_date = pd.to_datetime(df.current_date)\n",
    "\n",
    "dfA = df[[\"current_date\", \"payrate\"]].resample('1D', on=\"current_date\").cumsum()\n",
    "\n",
    "dfA\n",
    "\n",
    "# index = pd.date_range('1/1/2023', periods=52, freq='1W')\n",
    "\n",
    "# series = pd.Series(range(52), index=index)\n",
    "\n",
    "# series\n",
    "\n",
    "# series.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-0AW4763D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
